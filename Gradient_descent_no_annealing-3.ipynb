{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lVi-HvbPxtR2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import orth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxDfD1nOHJLM"
      },
      "source": [
        "# Algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s3Tknqv8yRHc"
      },
      "outputs": [],
      "source": [
        "def get_simplex_affine_space_projector(d):\n",
        "  A = np.zeros([d,d-1])\n",
        "  for i in range(d-1):\n",
        "    A[i,i] = 1.\n",
        "    A[i+1,i] = -1.\n",
        "  P = orth(A)\n",
        "  return P @ P.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "taTrKLyB0mlC"
      },
      "outputs": [],
      "source": [
        "def unit_simplex_projector(y):\n",
        "  r = len(y)\n",
        "  y_sorted = np.sort(y)\n",
        "\n",
        "  def criterion(mu):\n",
        "    res = np.sum(np.maximum(y_sorted - mu,0))\n",
        "    return res\n",
        "\n",
        "  idx = 0\n",
        "  while criterion(y_sorted[idx]) > 1:\n",
        "    idx += 1\n",
        "  if idx == 0:\n",
        "    mu_opt = (np.sum(y_sorted) - 1) / r\n",
        "  else:\n",
        "    mu_opt = (np.sum(y_sorted[idx:]) - 1) / (r - idx)\n",
        "\n",
        "  y_projected = np.maximum(y - mu_opt, 0)\n",
        "  return y_projected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nWYuCH2PzrxJ"
      },
      "outputs": [],
      "source": [
        "class WynerProblemSolver:\n",
        "\n",
        "  def __init__(self, p_joint, s, n_iters, **params):\n",
        "\n",
        "    self.p_joint = p_joint\n",
        "    self.n, self.m = p_joint.shape # support size for X and Y\n",
        "    self.s = s # support size for W\n",
        "    self.n_iters = n_iters\n",
        "    self.lr_init = params['lr_init'] if 'lr_init' in params.keys() else 1e-2\n",
        "    # beta - resularizer constant\n",
        "    self.beta_init = params['beta_init'] if 'beta_init' in params.keys() else 1.0\n",
        "    self.beta_decay_factor = params['beta_decay_factor'] if 'beta_decay_factor' in params.keys() else 2.0\n",
        "    self.beta_decay_every_iters = params['beta_decay_every_iters'] if 'beta_decay_every_iters' in params.keys() else 100\n",
        "    self.beta_min = params['beta_min'] if 'beta_min' in params.keys() else 1e-6\n",
        "    self.tol = params['tol'] if 'tol' in params.keys() else 1e-6 # tolerance in acceptance criterion\n",
        "    self.eps = params['eps'] if 'eps' in params.keys() else 1e-8 # smoothing const\n",
        "    self.print_every = params['print_every'] if 'print_every' in params.keys() else None\n",
        "    # parameter of Dirichlet distribution for parameters initialization\n",
        "    self.alpha = params['alpha'] if 'alpha' in params.keys() else 1\n",
        "    self.loss_type = params['loss_type'] if 'loss_type' in params.keys() else 'mle'\n",
        "    assert self.loss_type in ['mle', 'l2'] # Maximum likelyhood estimator and l2-norm loss\n",
        "    self.optim = params['optim'] if 'optim' in params.keys() else 'GD'\n",
        "    # GD - Gradient descent, LD - Langevin Dynamics, reLD - Replica exchange Langevin dynamics.\n",
        "    # reLD is not implemented yet\n",
        "    assert self.optim in ['GD', 'LD', 'reLD']\n",
        "    self.tau1 = params['tau1'] if 'tau1' in params.keys() else 1 # tempetature for LD and reLD\n",
        "    self.tau2 = params['tau2'] if 'tau2' in params.keys() else 10 # tempetature for reLD only\n",
        "    self.tau_decay_factor = params['tau_decay_factor'] if 'tau_decay_factor' in params.keys() else None\n",
        "    self.vanish_tau_after_iter = params['vanish_tau_after_iter'] if 'vanish_tau_after_iter' in params.keys() else None\n",
        "\n",
        "    # to be initialized in self.train() method\n",
        "    self.lambdas = None\n",
        "    self.X = None\n",
        "    self.Y = None\n",
        "\n",
        "    self.lambdas_best = None\n",
        "    self.X_best = None\n",
        "    self.Y_best = None\n",
        "    self.score_best = None\n",
        "\n",
        "    self.q_joint = None\n",
        "    self.lr = None\n",
        "\n",
        "    self.p_joint_entropy = - np.sum(np.where(p_joint != 0,p_joint * np.log(p_joint), 0))\n",
        "\n",
        "    self.lambdas_affine_projector = get_simplex_affine_space_projector(self.s)\n",
        "    self.X_affine_projector = get_simplex_affine_space_projector(self.n)\n",
        "    self.Y_affine_projector = get_simplex_affine_space_projector(self.m)\n",
        "\n",
        "    self.metrics = {\n",
        "        'iter': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "  def initialize(self):\n",
        "    self.lambdas = np.random.dirichlet([self.alpha] * self.s)\n",
        "    assert math.isclose(np.sum(self.lambdas), 1)\n",
        "\n",
        "    self.X = np.random.dirichlet([self.alpha] * self.n, (self.s,))\n",
        "    assert np.allclose(self.X.sum(axis=1), 1)\n",
        "\n",
        "    self.Y = np.random.dirichlet([self.alpha] * self.m, (self.s,))\n",
        "    assert np.allclose(self.Y.sum(axis=1), 1)\n",
        "\n",
        "  def eval_MLE_loss(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      q_joint = self.q_joint\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "      q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    res = (-1) * np.sum(self.p_joint * np.log(q_joint+self.eps))\n",
        "    return res\n",
        "\n",
        "  def eval_L2_loss(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      q_joint = self.q_joint\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "      q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    res = 0.5 * np.sum(np.square(self.p_joint - q_joint))\n",
        "    return res\n",
        "\n",
        "  def eval_D_KL(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      q_joint = self.q_joint\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "      q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    res = (-1) * np.sum(np.where(self.p_joint != 0, self.p_joint * np.log(q_joint), 0)) - self.p_joint_entropy\n",
        "    return res\n",
        "\n",
        "  def eval_objective_loss(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      lambdas, X, Y = self.lambdas, self.X, self.Y\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "    res = np.sum(lambdas * (np.sum(X * np.log(X+self.eps), axis=1) + np.sum(Y * np.log(Y+self.eps), axis=1)))\n",
        "    return res\n",
        "\n",
        "  def eval_constraint_loss(self, *model_params):\n",
        "    res = self.eval_MLE_loss(*model_params) if self.loss_type == 'mle' else self.eval_L2_loss(*model_params)\n",
        "    return res\n",
        "\n",
        "  def eval_final_objective(self, *model_params):\n",
        "    res = self.eval_constraint_loss(*model_params) + self.beta_min * self.eval_objective_loss(*model_params)\n",
        "    return res\n",
        "\n",
        "  def eval_distribution_proximity(self, *model_params):\n",
        "    res = self.eval_D_KL(*model_params) if self.loss_type == 'mle' else self.eval_L2_loss(*model_params)\n",
        "    return res\n",
        "\n",
        "  def eval_MLE_grads(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      lambdas, X, Y = self.lambdas, self.X, self.Y\n",
        "      q_joint = self.q_joint\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "      q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    p_div_q = self.p_joint / (q_joint + self.eps)\n",
        "    lambdas_grad = (-1) * np.sum(p_div_q * (X.reshape(self.s,self.n,1) * Y.reshape(self.s,1,self.m)), axis=(1,2))\n",
        "    X_grad = (-1) * np.sum(p_div_q.reshape(1,self.n,self.m) * (lambdas.reshape(self.s,1) * Y).reshape(self.s,1,self.m), axis=2)\n",
        "    Y_grad = (-1) * np.sum(p_div_q.reshape(1,self.n,self.m) * (lambdas.reshape(self.s,1) * X).reshape(self.s,self.n,1), axis=1)\n",
        "    return lambdas_grad, X_grad, Y_grad\n",
        "\n",
        "  def eval_L2_grads(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      lambdas, X, Y = self.lambdas, self.X, self.Y\n",
        "      q_joint = self.q_joint\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "      q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    lambdas_grad = np.sum((q_joint - self.p_joint) * (X.reshape(self.s,self.n,1) * Y.reshape(self.s,1,self.m)), axis=(1,2))\n",
        "    X_grad = np.sum((q_joint - self.p_joint).reshape(1,self.n,self.m) * (lambdas.reshape(self.s,1) * Y).reshape(self.s,1,self.m), axis=2)\n",
        "    Y_grad = np.sum((q_joint - self.p_joint).reshape(1,self.n,self.m) * (lambdas.reshape(self.s,1) * X).reshape(self.s,self.n,1), axis=1)\n",
        "    return lambdas_grad, X_grad, Y_grad\n",
        "\n",
        "  def eval_objective_grads(self, *model_params):\n",
        "    if len(model_params) == 0:\n",
        "      lambdas, X, Y = self.lambdas, self.X, self.Y\n",
        "    else:\n",
        "      lambdas, X, Y = model_params\n",
        "    lambdas_grad = (np.sum(X * np.log(X + self.eps), axis=1) + np.sum(Y * np.log(Y + self.eps), axis=1))\n",
        "    X_grad = lambdas.reshape(self.s,1) * (1 + np.log(X + self.eps))\n",
        "    Y_grad = lambdas.reshape(self.s,1) * (1 + np.log(Y + self.eps))\n",
        "    return lambdas_grad, X_grad, Y_grad\n",
        "\n",
        "  def eval_constraint_grads(self, *model_params):\n",
        "    res = self.eval_MLE_grads(*model_params) if self.loss_type == 'mle' else self.eval_L2_grads(*model_params)\n",
        "    return res\n",
        "\n",
        "  def eval_affine_grad_norm(self, lambdas_grad, X_grad, Y_grad):\n",
        "    lambdas_affine_grad = self.lambdas_affine_projector @ lambdas_grad\n",
        "    X_affine_grad = (self.X_affine_projector @ X_grad.T).T\n",
        "    Y_affine_grad = (self.Y_affine_projector @ Y_grad.T).T\n",
        "\n",
        "    affine_grads = (lambdas_affine_grad, X_affine_grad, Y_affine_grad)\n",
        "    affine_grad_norm = math.sqrt(sum([np.sum(np.square(g)) for g in affine_grads]))\n",
        "\n",
        "    return affine_grad_norm\n",
        "\n",
        "  def gradient_descent_step(self, loss, grads, beta, n_iter):\n",
        "\n",
        "    grad_norm_squared = sum([np.sum(np.square(g)) for g in grads])\n",
        "    lambdas_grad, X_grad, Y_grad = grads\n",
        "    criterion = False\n",
        "\n",
        "    while not criterion:\n",
        "\n",
        "      lambdas_next_unprojected = self.lambdas - self.lr * lambdas_grad\n",
        "      lambdas_next = unit_simplex_projector(lambdas_next_unprojected)\n",
        "\n",
        "      X_next_unprojected = self.X - self.lr * X_grad\n",
        "      X_next = np.zeros_like(X_next_unprojected)\n",
        "      for i in range(s):\n",
        "        X_next[i] = unit_simplex_projector(X_next_unprojected[i])\n",
        "\n",
        "      Y_next_unprojected = self.Y - self.lr * Y_grad\n",
        "      Y_next = np.zeros_like(Y_next_unprojected)\n",
        "      for i in range(s):\n",
        "        Y_next[i] = unit_simplex_projector(Y_next_unprojected[i])\n",
        "\n",
        "      params_next = (lambdas_next, X_next, Y_next)\n",
        "      params_next_unprojected = (lambdas_next_unprojected, X_next_unprojected, Y_next_unprojected)\n",
        "      correction_norm_squared = sum([np.sum(np.square(p1-p2)) for p1, p2 in zip(params_next, params_next_unprojected)])\n",
        "\n",
        "      loss_next = self.eval_constraint_loss(*params_next) + \\\n",
        "                  beta * self.eval_objective_loss(*params_next)\n",
        "\n",
        "      criterion = loss_next <= loss - (self.lr/2) * grad_norm_squared + 1 / (2*self.lr) * correction_norm_squared + self.tol\n",
        "      if not criterion:\n",
        "        self.lr /= 2\n",
        "\n",
        "    self.metrics['iter'].append(n_iter+1)\n",
        "    self.metrics['lr'].append(self.lr)\n",
        "\n",
        "    self.lr *= 2\n",
        "    return params_next\n",
        "\n",
        "  def langevin_dynamics_step(self, grads, n_iter):\n",
        "\n",
        "    lambdas_grad, X_grad, Y_grad = grads\n",
        "\n",
        "    lambdas_unprojected = self.lambdas - self.lr * lambdas_grad + \\\n",
        "                          math.sqrt(2 * self.lr * self.tau1) * np.random.randn(*self.lambdas.shape)\n",
        "    lambdas_next = unit_simplex_projector(lambdas_unprojected)\n",
        "\n",
        "    X_next_unprojected = self.X - self.lr * X_grad + \\\n",
        "                          math.sqrt(2 * self.lr * self.tau1) * np.random.randn(*self.X.shape)\n",
        "    X_next = np.zeros_like(X_next_unprojected)\n",
        "    for i in range(s):\n",
        "      X_next[i] = unit_simplex_projector(X_next_unprojected[i])\n",
        "\n",
        "    Y_next_unprojected = self.Y - self.lr * Y_grad + \\\n",
        "                          math.sqrt(2 * self.lr * self.tau1) * np.random.randn(*self.Y.shape)\n",
        "    Y_next = np.zeros_like(Y_next_unprojected)\n",
        "    for i in range(s):\n",
        "      Y_next[i] = unit_simplex_projector(Y_next_unprojected[i])\n",
        "\n",
        "    if self.tau_decay_factor is not None:\n",
        "      self.tau1 /= self.tau_decay_factor\n",
        "\n",
        "    if (self.vanish_tau_after_iter is not None) and (n_iter+1 == self.vanish_tau_after_iter):\n",
        "      self.tau1 = 0\n",
        "\n",
        "    self.metrics['iter'].append(n_iter+1)\n",
        "    self.metrics['lr'].append(self.lr)\n",
        "\n",
        "    return lambdas_next, X_next, Y_next\n",
        "\n",
        "  def print_message(self, n_iter, beta, last_step_size, **params):\n",
        "\n",
        "    objective_loss = params['objective_loss'] if 'objective_loss' in params.keys() else self.eval_objective_loss()\n",
        "    disrtibution_proximity = self.eval_distribution_proximity()\n",
        "    constraint_grads = params['constraint_grads'] if 'constraint_grads' in params.keys() else self.eval_constraint_grads()\n",
        "    objective_grads = params['objective_grads'] if 'objective_grads' in params.keys() else self.eval_objective_grads()\n",
        "\n",
        "    constr_affine_grad_norm = self.eval_affine_grad_norm(*constraint_grads)\n",
        "    objective_afine_grad_norm = self.eval_affine_grad_norm(*objective_grads)\n",
        "\n",
        "    message = f'{n_iter: <5}: distr proximity = {disrtibution_proximity: .8f}, objective loss = {objective_loss: .6f}, \\\n",
        "consrt_aff_grad = {constr_affine_grad_norm:.6f}, obj_aff_grad = {objective_afine_grad_norm:.6f}, beta = {beta: .8f}'\n",
        "\n",
        "    if self.optim == 'GD':\n",
        "      message += f', lr = {self.lr: .6f}'\n",
        "    else:\n",
        "      message += f', tau1 = {self.tau1:.8f}'\n",
        "\n",
        "    if n_iter > 0:\n",
        "      message += f', step size = {last_step_size:.8f}'\n",
        "\n",
        "    print(message)\n",
        "\n",
        "  def train(self):\n",
        "    self.lr = self.lr_init\n",
        "    beta = self.beta_init\n",
        "\n",
        "    self.initialize()\n",
        "\n",
        "    self.q_joint = self.X.T @ np.diag(self.lambdas) @ self.Y\n",
        "\n",
        "    self.lambdas_best, self.X_best, self.Y_best = self.lambdas.copy(), self.X.copy(), self.Y.copy()\n",
        "    self.score_best = self.eval_final_objective()\n",
        "\n",
        "    last_step_size = None\n",
        "\n",
        "    for n_iter in range(self.n_iters):\n",
        "\n",
        "      constraint_loss = self.eval_constraint_loss()\n",
        "      objective_loss = self.eval_objective_loss()\n",
        "      loss = constraint_loss + beta * objective_loss\n",
        "\n",
        "      constraint_grads = self.eval_constraint_grads()\n",
        "      objective_grads = self.eval_objective_grads()\n",
        "      grads = [g1 + beta * g2 for g1, g2 in zip(constraint_grads, objective_grads)]\n",
        "\n",
        "      if (self.print_every is not None) and (n_iter % self.print_every == 0):\n",
        "        params = {\n",
        "            'objective_loss': objective_loss,\n",
        "            'constraint_grads': constraint_grads,\n",
        "            'objective_grads': objective_grads,\n",
        "        }\n",
        "        self.print_message(n_iter, beta, last_step_size, **params)\n",
        "\n",
        "      if self.optim == 'GD':\n",
        "        params_next = self.gradient_descent_step(loss, grads, beta, n_iter)\n",
        "      else:\n",
        "        params_next = self.langevin_dynamics_step(grads, n_iter)\n",
        "\n",
        "      params = (self.lambdas, self.X, self.Y)\n",
        "      last_step_size = math.sqrt(sum([np.sum(np.square(p1-p2)) for p1, p2 in zip(params, params_next)]))\n",
        "      self.lambdas, self.X, self.Y = params_next\n",
        "\n",
        "      new_score = self.eval_final_objective()\n",
        "      if new_score < self.score_best:\n",
        "        self.score_best = new_score\n",
        "        self.lambdas_best, self.X_best, self.Y_best = self.lambdas.copy(), self.X.copy(), self.Y.copy()\n",
        "\n",
        "      assert np.allclose(self.X.sum(axis=1), 1), f'{self.X.sum(axis=1)}'\n",
        "      assert np.allclose(self.Y.sum(axis=1), 1), f'{self.Y.sum(axis=1)}'\n",
        "      assert math.isclose(np.sum(self.lambdas), 1), f'{np.sum(self.lambdas)}'\n",
        "\n",
        "      self.q_joint = self.X.T @ np.diag(self.lambdas) @ self.Y\n",
        "\n",
        "      if (self.beta_decay_every_iters is not None) and (n_iter % self.beta_decay_every_iters == 0) and (n_iter > 0):\n",
        "        beta /= self.beta_decay_factor\n",
        "      if (self.beta_min is not None) and (beta < self.beta_min):\n",
        "        beta = self.beta_min\n",
        "\n",
        "    if self.print_every is not None:\n",
        "      self.print_message(self.n_iters, beta, last_step_size)\n",
        "\n",
        "    return self.lambdas_best, self.X_best, self.Y_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F90E-f17gtr7"
      },
      "source": [
        "# Doubly symmetric bynary sourse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61S1zETVru8_",
        "outputId": "18735aa1-05f7-4ac4-8fa0-c61db1705f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambdas_true:\n",
            "[0.5 0.5]\n",
            "X_true:\n",
            "[[0.78868 0.21132]\n",
            " [0.21132 0.78868]]\n",
            "Y_true:\n",
            "[[0.78868 0.21132]\n",
            " [0.21132 0.78868]]\n",
            "C(X,Y) =  0.43028\n"
          ]
        }
      ],
      "source": [
        "# DSBS test\n",
        "a0 = 1/3 # crossover prob\n",
        "a1 = 0.5 * (1 - math.sqrt(1-2*a0))\n",
        "p_XY = 0.5 * np.array([[1-a0, a0],\n",
        "                       [a0, 1-a0]])\n",
        "lambdas_true = np.array([1/2, 1/2])\n",
        "X_true = np.array([[1-a1, a1],\n",
        "                   [a1, 1-a1]])\n",
        "Y_true = np.array([[1-a1, a1],\n",
        "                   [a1, 1-a1]])\n",
        "def h(a):\n",
        "  return - (a * math.log2(a) + (1-a) * math.log2(1-a))\n",
        "\n",
        "C_XY_true = 1 + h(a0) - 2 * h(a1)\n",
        "\n",
        "p_XY_entropy = - np.sum(np.where(p_XY != 0,p_XY * np.log2(p_XY), 0))\n",
        "\n",
        "def evaluate_cond_entropy(lambdas, X, Y):\n",
        "    q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    res = (-1) * np.sum(lambdas * (np.sum(X * np.log2(X), axis=1) + np.sum(Y * np.log2(Y), axis=1)))\n",
        "    return res\n",
        "\n",
        "def evaluate_D_KL(lambdas, X, Y, p_joint):\n",
        "    q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    p_joiny_entropy = - np.sum(np.where(p_joint != 0,p_joint * np.log(p_joint), 0))\n",
        "    res = (-1) * np.sum(np.where(p_joint != 0, p_joint * np.log(q_joint), 0)) - p_joiny_entropy\n",
        "    return res\n",
        "\n",
        "with np.printoptions(precision=5):\n",
        "  print(f'lambdas_true:\\n{lambdas_true}')\n",
        "  print(f'X_true:\\n{X_true}')\n",
        "  print(f'Y_true:\\n{Y_true}')\n",
        "\n",
        "print(f'C(X,Y) = {C_XY_true: .5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY53GT4f-zPs",
        "outputId": "fffee87f-b9d4-4002-f640-5b4c09b6e606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    : distr proximity =  0.06882138, objective loss = -1.033665, consrt_aff_grad = 0.254843, obj_aff_grad = 1.403484, beta =  0.10000000, lr =  0.010000\n",
            "100  : distr proximity =  0.01384705, objective loss = -1.385791, consrt_aff_grad = 0.001510, obj_aff_grad = 0.016926, beta =  0.10000000, lr =  2.560000, step size = 0.00060974\n",
            "200  : distr proximity =  0.00554996, objective loss = -1.260274, consrt_aff_grad = 0.025693, obj_aff_grad = 0.513899, beta =  0.05000000, lr =  10.240000, step size = 0.00257083\n",
            "300  : distr proximity =  0.00147923, objective loss = -1.152337, consrt_aff_grad = 0.018222, obj_aff_grad = 0.728881, beta =  0.02500000, lr =  5.120000, step size = 0.00053022\n",
            "400  : distr proximity =  0.00038652, objective loss = -1.093985, consrt_aff_grad = 0.010361, obj_aff_grad = 0.828898, beta =  0.01250000, lr =  5.120000, step size = 0.00027001\n",
            "500  : distr proximity =  0.00009902, objective loss = -1.063284, consrt_aff_grad = 0.005498, obj_aff_grad = 0.879578, beta =  0.00625000, lr =  5.120000, step size = 0.00018363\n",
            "600  : distr proximity =  0.00002508, objective loss = -1.047500, consrt_aff_grad = 0.002829, obj_aff_grad = 0.905274, beta =  0.00312500, lr =  5.120000, step size = 0.00021561\n",
            "700  : distr proximity =  0.00000631, objective loss = -1.039493, consrt_aff_grad = 0.001436, obj_aff_grad = 0.918233, beta =  0.00156250, lr =  10.240000, step size = 0.00015088\n",
            "800  : distr proximity =  0.00000159, objective loss = -1.035459, consrt_aff_grad = 0.000727, obj_aff_grad = 0.924743, beta =  0.00078125, lr =  10.240000, step size = 0.00020328\n",
            "900  : distr proximity =  0.00000040, objective loss = -1.033435, consrt_aff_grad = 0.000367, obj_aff_grad = 0.928007, beta =  0.00039063, lr =  10.240000, step size = 0.00014690\n",
            "1000 : distr proximity =  0.00000010, objective loss = -1.032421, consrt_aff_grad = 0.000191, obj_aff_grad = 0.929640, beta =  0.00019531, lr =  10.240000, step size = 0.00015177\n",
            "1100 : distr proximity =  0.00000003, objective loss = -1.031913, consrt_aff_grad = 0.000117, obj_aff_grad = 0.930457, beta =  0.00009766, lr =  10.240000, step size = 0.00018896\n",
            "1200 : distr proximity =  0.00000001, objective loss = -1.031659, consrt_aff_grad = 0.000052, obj_aff_grad = 0.930868, beta =  0.00004883, lr =  5.120000, step size = 0.00012919\n",
            "1300 : distr proximity =  0.00000000, objective loss = -1.031532, consrt_aff_grad = 0.000042, obj_aff_grad = 0.931072, beta =  0.00002441, lr =  5.120000, step size = 0.00018491\n",
            "1400 : distr proximity =  0.00000000, objective loss = -1.031469, consrt_aff_grad = 0.000053, obj_aff_grad = 0.931173, beta =  0.00001221, lr =  10.240000, step size = 0.00013447\n",
            "1500 : distr proximity =  0.00000001, objective loss = -1.031437, consrt_aff_grad = 0.000077, obj_aff_grad = 0.931224, beta =  0.00000610, lr =  10.240000, step size = 0.00019902\n",
            "1600 : distr proximity =  0.00000000, objective loss = -1.031421, consrt_aff_grad = 0.000029, obj_aff_grad = 0.931251, beta =  0.00000305, lr =  5.120000, step size = 0.00014816\n",
            "1700 : distr proximity =  0.00000000, objective loss = -1.031413, consrt_aff_grad = 0.000043, obj_aff_grad = 0.931263, beta =  0.00000153, lr =  5.120000, step size = 0.00022123\n",
            "1800 : distr proximity =  0.00000000, objective loss = -1.031409, consrt_aff_grad = 0.000064, obj_aff_grad = 0.931269, beta =  0.00000076, lr =  10.240000, step size = 0.00016430\n",
            "1900 : distr proximity =  0.00000000, objective loss = -1.031407, consrt_aff_grad = 0.000024, obj_aff_grad = 0.931273, beta =  0.00000038, lr =  5.120000, step size = 0.00012291\n",
            "2000 : distr proximity =  0.00000000, objective loss = -1.031406, consrt_aff_grad = 0.000035, obj_aff_grad = 0.931274, beta =  0.00000019, lr =  5.120000, step size = 0.00018399\n",
            "2100 : distr proximity =  0.00000000, objective loss = -1.031406, consrt_aff_grad = 0.000053, obj_aff_grad = 0.931275, beta =  0.00000010, lr =  10.240000, step size = 0.00013681\n",
            "2200 : distr proximity =  0.00000001, objective loss = -1.031406, consrt_aff_grad = 0.000079, obj_aff_grad = 0.931274, beta =  0.00000010, lr =  10.240000, step size = 0.00020481\n",
            "2300 : distr proximity =  0.00000000, objective loss = -1.031406, consrt_aff_grad = 0.000030, obj_aff_grad = 0.931275, beta =  0.00000010, lr =  5.120000, step size = 0.00015331\n",
            "2400 : distr proximity =  0.00000000, objective loss = -1.031406, consrt_aff_grad = 0.000044, obj_aff_grad = 0.931275, beta =  0.00000010, lr =  10.240000, step size = 0.00011401\n",
            "2500 : distr proximity =  0.00000000, objective loss = -1.031406, consrt_aff_grad = 0.000066, obj_aff_grad = 0.931274, beta =  0.00000010, lr =  10.240000, step size = 0.00017067\n",
            "C_XY_estim = 0.43029\n"
          ]
        }
      ],
      "source": [
        "p_joint=p_XY\n",
        "s=2\n",
        "n_iters=2500\n",
        "params = {}\n",
        "params['lr_init']=0.01\n",
        "params['beta_init']=1e-1\n",
        "params['beta_decay_factor']=2\n",
        "params['beta_decay_every_iters']=100\n",
        "params['beta_min'] = 1e-7\n",
        "params['tol']=1e-8\n",
        "params['eps']=1e-8\n",
        "params['print_every'] = 100\n",
        "params['alpha'] = 1\n",
        "params['loss_type'] = 'l2'\n",
        "params['optim'] = 'GD'\n",
        "params['tau1'] = 1\n",
        "params['tau2'] = 10\n",
        "\n",
        "solver_GD = WynerProblemSolver(p_joint, s, n_iters, **params)\n",
        "lambdas, X, Y = solver_GD.train()\n",
        "\n",
        "cond_entropy = evaluate_cond_entropy(lambdas, X, Y)\n",
        "C_XY_estim = p_XY_entropy - cond_entropy\n",
        "print(f'C_XY_estim = {C_XY_estim:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZWbdDz9CMhc",
        "outputId": "d91854f8-deba-45cd-a74d-580d498f9226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    : distr proximity =  0.02514112, objective loss = -1.103744, consrt_aff_grad = 0.139611, obj_aff_grad = 1.018792, beta =  0.10000000, tau1 = 0.00000000\n",
            "100  : distr proximity =  0.01370050, objective loss = -1.384022, consrt_aff_grad = 0.005372, obj_aff_grad = 0.063827, beta =  0.10000000, tau1 = 0.00000000, step size = 0.00108117\n",
            "200  : distr proximity =  0.00636021, objective loss = -1.276128, consrt_aff_grad = 0.025859, obj_aff_grad = 0.478843, beta =  0.05000000, tau1 = 0.00000000, step size = 0.00204200\n",
            "300  : distr proximity =  0.00148838, objective loss = -1.152160, consrt_aff_grad = 0.018257, obj_aff_grad = 0.730249, beta =  0.02500000, tau1 = 0.00000000, step size = 0.00029205\n",
            "400  : distr proximity =  0.00038864, objective loss = -1.093553, consrt_aff_grad = 0.010394, obj_aff_grad = 0.831580, beta =  0.01250000, tau1 = 0.00000000, step size = 0.00017211\n",
            "500  : distr proximity =  0.00009949, objective loss = -1.062787, consrt_aff_grad = 0.005515, obj_aff_grad = 0.882493, beta =  0.00625000, tau1 = 0.00000000, step size = 0.00009018\n",
            "600  : distr proximity =  0.00002518, objective loss = -1.046989, consrt_aff_grad = 0.002838, obj_aff_grad = 0.908207, beta =  0.00312500, tau1 = 0.00000000, step size = 0.00004578\n",
            "700  : distr proximity =  0.00000634, objective loss = -1.038978, consrt_aff_grad = 0.001439, obj_aff_grad = 0.921155, beta =  0.00156250, tau1 = 0.00000000, step size = 0.00002302\n",
            "800  : distr proximity =  0.00000159, objective loss = -1.034943, consrt_aff_grad = 0.000725, obj_aff_grad = 0.927655, beta =  0.00078125, tau1 = 0.00000000, step size = 0.00001154\n",
            "900  : distr proximity =  0.00000040, objective loss = -1.032919, consrt_aff_grad = 0.000364, obj_aff_grad = 0.930911, beta =  0.00039063, tau1 = 0.00000000, step size = 0.00000577\n",
            "1000 : distr proximity =  0.00000010, objective loss = -1.031904, consrt_aff_grad = 0.000182, obj_aff_grad = 0.932542, beta =  0.00019531, tau1 = 0.00000000, step size = 0.00000289\n",
            "1100 : distr proximity =  0.00000002, objective loss = -1.031397, consrt_aff_grad = 0.000091, obj_aff_grad = 0.933357, beta =  0.00009766, tau1 = 0.00000000, step size = 0.00000144\n",
            "1200 : distr proximity =  0.00000001, objective loss = -1.031143, consrt_aff_grad = 0.000046, obj_aff_grad = 0.933765, beta =  0.00004883, tau1 = 0.00000000, step size = 0.00000072\n",
            "1300 : distr proximity =  0.00000000, objective loss = -1.031016, consrt_aff_grad = 0.000023, obj_aff_grad = 0.933969, beta =  0.00002441, tau1 = 0.00000000, step size = 0.00000036\n",
            "1400 : distr proximity =  0.00000000, objective loss = -1.030953, consrt_aff_grad = 0.000011, obj_aff_grad = 0.934071, beta =  0.00001221, tau1 = 0.00000000, step size = 0.00000018\n",
            "1500 : distr proximity =  0.00000000, objective loss = -1.030921, consrt_aff_grad = 0.000006, obj_aff_grad = 0.934122, beta =  0.00000610, tau1 = 0.00000000, step size = 0.00000009\n",
            "1600 : distr proximity =  0.00000000, objective loss = -1.030905, consrt_aff_grad = 0.000003, obj_aff_grad = 0.934147, beta =  0.00000305, tau1 = 0.00000000, step size = 0.00000005\n",
            "1700 : distr proximity =  0.00000000, objective loss = -1.030897, consrt_aff_grad = 0.000001, obj_aff_grad = 0.934160, beta =  0.00000153, tau1 = 0.00000000, step size = 0.00000002\n",
            "1800 : distr proximity =  0.00000000, objective loss = -1.030893, consrt_aff_grad = 0.000001, obj_aff_grad = 0.934167, beta =  0.00000076, tau1 = 0.00000000, step size = 0.00000001\n",
            "1900 : distr proximity =  0.00000000, objective loss = -1.030891, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934170, beta =  0.00000038, tau1 = 0.00000000, step size = 0.00000001\n",
            "2000 : distr proximity =  0.00000000, objective loss = -1.030890, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934171, beta =  0.00000019, tau1 = 0.00000000, step size = 0.00000000\n",
            "2100 : distr proximity =  0.00000000, objective loss = -1.030890, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934172, beta =  0.00000010, tau1 = 0.00000000, step size = 0.00000000\n",
            "2200 : distr proximity =  0.00000000, objective loss = -1.030890, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934172, beta =  0.00000010, tau1 = 0.00000000, step size = 0.00000000\n",
            "2300 : distr proximity =  0.00000000, objective loss = -1.030890, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934172, beta =  0.00000010, tau1 = 0.00000000, step size = 0.00000000\n",
            "2400 : distr proximity =  0.00000000, objective loss = -1.030890, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934172, beta =  0.00000010, tau1 = 0.00000000, step size = 0.00000000\n",
            "2500 : distr proximity =  0.00000000, objective loss = -1.030890, consrt_aff_grad = 0.000000, obj_aff_grad = 0.934172, beta =  0.00000010, tau1 = 0.00000000, step size = 0.00000000\n",
            "C_XY_estim = 0.43104\n"
          ]
        }
      ],
      "source": [
        "p_joint=p_XY\n",
        "s=2\n",
        "n_iters=2500\n",
        "params = {}\n",
        "params['lr_init']=1\n",
        "params['beta_init']=1e-1\n",
        "params['beta_decay_factor']=2\n",
        "params['beta_decay_every_iters']=100\n",
        "params['beta_min'] = 1e-7\n",
        "params['tol']=1e-8\n",
        "params['eps']=1e-8\n",
        "params['print_every'] = 100\n",
        "params['alpha'] = 1\n",
        "params['loss_type'] = 'l2'\n",
        "params['optim'] = 'LD'\n",
        "params['tau1'] = 0\n",
        "params['tau2'] = 10\n",
        "params['tau_decay_factor'] = 1.0001\n",
        "params['vanish_tau_after_iter'] = 16000\n",
        "\n",
        "solver_LD = WynerProblemSolver(p_joint, s, n_iters, **params)\n",
        "lambdas, X, Y = solver_LD.train()\n",
        "\n",
        "cond_entropy = evaluate_cond_entropy(lambdas, X, Y)\n",
        "C_XY_estim = p_XY_entropy - cond_entropy\n",
        "print(f'C_XY_estim = {C_XY_estim:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "lt9D711wD1jg",
        "outputId": "b5407105-59bf-451e-fbcf-50359e705465"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(solver_GD.metrics['iter'], solver_GD.metrics['lr'], label='adaptive lr', s=1)\n",
        "plt.scatter(solver_LD.metrics['iter'], solver_LD.metrics['lr'], label='const lr', s=1)\n",
        "\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('learning rate')\n",
        "#plt.yscale('log')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ4S9wKp5EyP",
        "outputId": "1cf27a71-d7b7-4f5f-d434-4cdf52eeb051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proximity =  0.00000000, lambda_min =  0.49527, C_XY_estim - C_XY_true =  0.000014\n",
            "proximity =  0.00000000, lambda_min =  0.49582, C_XY_estim - C_XY_true =  0.000012\n",
            "proximity =  0.00000000, lambda_min =  0.49793, C_XY_estim - C_XY_true =  0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49899, C_XY_estim - C_XY_true = -0.000000\n",
            "proximity =  0.00000000, lambda_min =  0.49699, C_XY_estim - C_XY_true =  0.000005\n",
            "proximity =  0.00000000, lambda_min =  0.49561, C_XY_estim - C_XY_true =  0.000010\n",
            "proximity =  0.00000000, lambda_min =  0.49779, C_XY_estim - C_XY_true =  0.000003\n",
            "proximity =  0.00000000, lambda_min =  0.49546, C_XY_estim - C_XY_true =  0.000013\n",
            "proximity =  0.00000000, lambda_min =  0.49524, C_XY_estim - C_XY_true =  0.000014\n",
            "proximity =  0.00000000, lambda_min =  0.49801, C_XY_estim - C_XY_true =  0.000002\n",
            "proximity =  0.00000000, lambda_min =  0.49714, C_XY_estim - C_XY_true =  0.000004\n",
            "proximity =  0.00000000, lambda_min =  0.49891, C_XY_estim - C_XY_true =  0.000000\n",
            "proximity =  0.00000000, lambda_min =  0.49970, C_XY_estim - C_XY_true = -0.000006\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.49640, C_XY_estim - C_XY_true =  0.000008\n",
            "proximity =  0.00000000, lambda_min =  0.49733, C_XY_estim - C_XY_true =  0.000004\n",
            "proximity =  0.00000000, lambda_min =  0.49638, C_XY_estim - C_XY_true =  0.000006\n",
            "proximity =  0.00000000, lambda_min =  0.49604, C_XY_estim - C_XY_true =  0.000011\n",
            "proximity =  0.00000000, lambda_min =  0.49680, C_XY_estim - C_XY_true =  0.000006\n",
            "proximity =  0.00000000, lambda_min =  0.49809, C_XY_estim - C_XY_true =  0.000002\n",
            "proximity =  0.00000000, lambda_min =  0.49576, C_XY_estim - C_XY_true =  0.000011\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.49847, C_XY_estim - C_XY_true =  0.000000\n",
            "proximity =  0.00000000, lambda_min =  0.49954, C_XY_estim - C_XY_true = -0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49930, C_XY_estim - C_XY_true = -0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49683, C_XY_estim - C_XY_true =  0.000006\n",
            "proximity =  0.00000000, lambda_min =  0.49679, C_XY_estim - C_XY_true =  0.000007\n",
            "proximity =  0.00000000, lambda_min =  0.49615, C_XY_estim - C_XY_true =  0.000010\n",
            "proximity =  0.00000000, lambda_min =  0.49862, C_XY_estim - C_XY_true =  0.000000\n",
            "proximity =  0.00000000, lambda_min =  0.49577, C_XY_estim - C_XY_true =  0.000012\n",
            "proximity =  0.00000000, lambda_min =  0.49865, C_XY_estim - C_XY_true = -0.000005\n",
            "proximity =  0.00000000, lambda_min =  0.49812, C_XY_estim - C_XY_true = -0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49770, C_XY_estim - C_XY_true =  0.000003\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.49615, C_XY_estim - C_XY_true =  0.000009\n",
            "proximity =  0.00000000, lambda_min =  0.49623, C_XY_estim - C_XY_true =  0.000009\n",
            "proximity =  0.00000000, lambda_min =  0.49907, C_XY_estim - C_XY_true = -0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49789, C_XY_estim - C_XY_true =  0.000003\n",
            "proximity =  0.00000000, lambda_min =  0.49852, C_XY_estim - C_XY_true =  0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49834, C_XY_estim - C_XY_true =  0.000000\n",
            "proximity =  0.00000000, lambda_min =  0.49639, C_XY_estim - C_XY_true =  0.000009\n",
            "proximity =  0.00000000, lambda_min =  0.49985, C_XY_estim - C_XY_true = -0.000001\n",
            "proximity =  0.00000000, lambda_min =  0.49693, C_XY_estim - C_XY_true =  0.000004\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.46410, C_XY_estim - C_XY_true =  0.000891\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.49738, C_XY_estim - C_XY_true =  0.000002\n",
            "proximity =  0.00000000, lambda_min =  0.49661, C_XY_estim - C_XY_true =  0.000008\n",
            "proximity =  0.00000000, lambda_min =  0.49823, C_XY_estim - C_XY_true =  0.000001\n"
          ]
        }
      ],
      "source": [
        "p_joint=p_XY\n",
        "s=2\n",
        "n_iters=2500\n",
        "params = {}\n",
        "params['lr_init']=0.01\n",
        "params['beta_init']=0.1\n",
        "params['beta_decay_factor']=2\n",
        "params['beta_decay_every_iters']=100\n",
        "params['beta_min'] = 1e-7\n",
        "params['tol']=1e-8\n",
        "params['eps']=1e-8\n",
        "params['print_every'] = None\n",
        "params['alpha'] = 1\n",
        "params['loss_type'] = 'l2'\n",
        "params['optim'] = 'GD'\n",
        "params['tau1'] = 1\n",
        "params['tau2'] = 10\n",
        "\n",
        "lambdas_min_GD = []\n",
        "C_XY_estim_GD = []\n",
        "proximity_GD = []\n",
        "for _ in range(50):\n",
        "  solver_GD = WynerProblemSolver(p_joint, s, n_iters, **params)\n",
        "  lambdas, X, Y = solver_GD.train()\n",
        "  lambdas_min_GD.append(lambdas.min())\n",
        "  cond_entropy = evaluate_cond_entropy(lambdas, X, Y)\n",
        "  C_XY_estim_GD.append(p_XY_entropy - cond_entropy)\n",
        "  proximity_GD.append(solver_GD.eval_distribution_proximity(lambdas, X, Y))\n",
        "  print(f'proximity = {proximity_GD[-1]: .8f}, lambda_min = {lambdas_min_GD[-1]: .5f}, C_XY_estim - C_XY_true = {C_XY_estim_GD[-1] - C_XY_true: .6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHLyT3J3HoR1",
        "outputId": "593f87bc-2db8-4212-d363-090c4d0dbe18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision = 0.000025, recall = 88.0%\n"
          ]
        }
      ],
      "source": [
        "proximity_GD = np.array(proximity_GD)\n",
        "C_XY_estim_GD = np.array(C_XY_estim_GD)\n",
        "\n",
        "mask = proximity_GD < 1e-7\n",
        "\n",
        "recall = np.sum(mask) / len(proximity_GD)\n",
        "precision = np.mean(C_XY_estim_GD[mask] - C_XY_true)\n",
        "\n",
        "print(f'precision = {precision:.6f}, recall = {100*recall}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lMXbFYIVbYl",
        "outputId": "2834e413-2811-4dbc-a89c-3607294ab465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proximity =  0.00000000, lambda_min =  0.34494, C_XY_estim - C_XY_true =  0.027703\n",
            "proximity =  0.00000000, lambda_min =  0.45349, C_XY_estim - C_XY_true =  0.001529\n",
            "proximity =  0.00000000, lambda_min =  0.41535, C_XY_estim - C_XY_true =  0.005256\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.43608, C_XY_estim - C_XY_true =  0.002935\n",
            "proximity =  0.00000000, lambda_min =  0.33958, C_XY_estim - C_XY_true =  0.024271\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.49082, C_XY_estim - C_XY_true =  0.000058\n",
            "proximity =  0.00000000, lambda_min =  0.44167, C_XY_estim - C_XY_true =  0.002421\n",
            "proximity =  0.00000000, lambda_min =  0.35141, C_XY_estim - C_XY_true =  0.022762\n",
            "proximity =  0.00000000, lambda_min =  0.48450, C_XY_estim - C_XY_true =  0.000172\n",
            "proximity =  0.00000000, lambda_min =  0.44947, C_XY_estim - C_XY_true =  0.001837\n",
            "proximity =  0.00000000, lambda_min =  0.48499, C_XY_estim - C_XY_true =  0.000171\n",
            "proximity =  0.00000000, lambda_min =  0.48051, C_XY_estim - C_XY_true =  0.000263\n",
            "proximity =  0.00000000, lambda_min =  0.47354, C_XY_estim - C_XY_true =  0.000486\n",
            "proximity =  0.00000000, lambda_min =  0.47869, C_XY_estim - C_XY_true =  0.000329\n",
            "proximity =  0.00000000, lambda_min =  0.39864, C_XY_estim - C_XY_true =  0.007743\n",
            "proximity =  0.00000000, lambda_min =  0.46425, C_XY_estim - C_XY_true =  0.000897\n",
            "proximity =  0.00000000, lambda_min =  0.48732, C_XY_estim - C_XY_true =  0.000123\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.41303, C_XY_estim - C_XY_true =  0.005577\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.39149, C_XY_estim - C_XY_true =  0.009221\n",
            "proximity =  0.00000000, lambda_min =  0.34797, C_XY_estim - C_XY_true =  0.020210\n",
            "proximity =  0.00000000, lambda_min =  0.45489, C_XY_estim - C_XY_true =  0.001432\n",
            "proximity =  0.00000000, lambda_min =  0.49238, C_XY_estim - C_XY_true =  0.000047\n",
            "proximity =  0.00000000, lambda_min =  0.41409, C_XY_estim - C_XY_true =  0.005423\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.37376, C_XY_estim - C_XY_true =  0.012677\n",
            "proximity =  0.00000000, lambda_min =  0.47363, C_XY_estim - C_XY_true =  0.000483\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.42291, C_XY_estim - C_XY_true =  0.004408\n",
            "proximity =  0.00000000, lambda_min =  0.34644, C_XY_estim - C_XY_true =  0.020499\n",
            "proximity =  0.00000000, lambda_min =  0.44276, C_XY_estim - C_XY_true =  0.002326\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.47824, C_XY_estim - C_XY_true =  0.000347\n",
            "proximity =  0.00000000, lambda_min =  0.42534, C_XY_estim - C_XY_true =  0.004034\n",
            "proximity =  0.00000000, lambda_min =  0.38421, C_XY_estim - C_XY_true =  0.010675\n",
            "proximity =  0.00000000, lambda_min =  0.37670, C_XY_estim - C_XY_true =  0.012303\n",
            "proximity =  0.00000000, lambda_min =  0.34015, C_XY_estim - C_XY_true =  0.023982\n",
            "proximity =  0.00000000, lambda_min =  0.36058, C_XY_estim - C_XY_true =  0.016000\n",
            "proximity =  0.00000000, lambda_min =  0.35481, C_XY_estim - C_XY_true =  0.019618\n",
            "proximity =  0.00000000, lambda_min =  0.34274, C_XY_estim - C_XY_true =  0.025849\n",
            "proximity =  0.00000000, lambda_min =  0.30719, C_XY_estim - C_XY_true =  0.037344\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n",
            "proximity =  0.00000000, lambda_min =  0.37934, C_XY_estim - C_XY_true =  0.011409\n",
            "proximity =  0.00000000, lambda_min =  0.43839, C_XY_estim - C_XY_true =  0.002723\n",
            "proximity =  0.01388889, lambda_min =  0.00000, C_XY_estim - C_XY_true = -0.511985\n"
          ]
        }
      ],
      "source": [
        "p_joint=p_XY\n",
        "s=2\n",
        "n_iters=2500\n",
        "params = {}\n",
        "params['lr_init']=0.1\n",
        "params['beta_init']=1e-1\n",
        "params['beta_decay_factor']=2\n",
        "params['beta_decay_every_iters']=100\n",
        "params['beta_min'] = 1e-7\n",
        "params['tol']=1e-8\n",
        "params['eps']=1e-8\n",
        "params['print_every'] = None\n",
        "params['alpha'] = 1\n",
        "params['loss_type'] = 'l2'\n",
        "params['optim'] = 'LD'\n",
        "params['tau1'] = 0\n",
        "params['tau2'] = 10\n",
        "params['tau_decay_factor'] = 1.0001\n",
        "params['vanish_tau_after_iter'] = 16000\n",
        "\n",
        "lambdas_min_LD = []\n",
        "C_XY_estim_LD = []\n",
        "proximity_LD = []\n",
        "for _ in range(50):\n",
        "  solver_LD = WynerProblemSolver(p_joint, s, n_iters, **params)\n",
        "  lambdas, X, Y = solver_LD.train()\n",
        "  lambdas_min_LD.append(lambdas.min())\n",
        "  cond_entropy = evaluate_cond_entropy(lambdas, X, Y)\n",
        "  C_XY_estim_LD.append(p_XY_entropy - cond_entropy)\n",
        "  proximity_LD.append(solver_LD.eval_distribution_proximity(lambdas, X, Y))\n",
        "  print(f'proximity = {proximity_LD[-1]: .8f}, lambda_min = {lambdas_min_LD[-1]: .5f}, C_XY_estim - C_XY_true = {C_XY_estim_LD[-1] - C_XY_true: .6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAiZe_ySKIUU",
        "outputId": "80de3e38-1dcb-4a13-dc85-cb81c9498d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision = 0.008860, recall = 78.0%\n"
          ]
        }
      ],
      "source": [
        "proximity_LD = np.array(proximity_LD)\n",
        "C_XY_estim_LD = np.array(C_XY_estim_LD)\n",
        "\n",
        "mask = proximity_LD < 1e-7\n",
        "\n",
        "recall = np.sum(mask) / len(proximity_LD)\n",
        "precision = np.mean(C_XY_estim_LD[mask] - C_XY_true)\n",
        "\n",
        "print(f'precision = {precision:.6f}, recall = {100*recall}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ1Dwb7UiLpJ"
      },
      "source": [
        "# Combination of doubly symmetric bynary sourses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCZuWVVEiPI0"
      },
      "source": [
        "## k = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-S8UMC_Nihe1"
      },
      "outputs": [],
      "source": [
        "alpha_1, alpha_2 = 0.2, 0.4\n",
        "a1 = 0.5 * (1 - math.sqrt(1-2*alpha_1))\n",
        "a2 = 0.5 * (1 - math.sqrt(1-2*alpha_2))\n",
        "\n",
        "p_XY_1 = 0.5 * np.array([[1-alpha_1, alpha_1],\n",
        "                       [alpha_1, 1-alpha_1]])\n",
        "lambdas_1 = np.array([1/2, 1/2])\n",
        "X_1 = np.array([[1-a1, a1],\n",
        "                   [a1, 1-a1]])\n",
        "Y_1 = np.array([[1-a1, a1],\n",
        "                   [a1, 1-a1]])\n",
        "\n",
        "p_XY_2 = 0.5 * np.array([[1-alpha_2, alpha_2],\n",
        "                       [alpha_2, 1-alpha_2]])\n",
        "lambdas_2 = np.array([1/2, 1/2])\n",
        "X_2 = np.array([[1-a2, a2],\n",
        "                   [a2, 1-a2]])\n",
        "Y_2 = np.array([[1-a2, a2],\n",
        "                   [a2, 1-a2]])\n",
        "\n",
        "p_XY = np.zeros([4,4])\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    x1 = i // 2\n",
        "    x2 = i % 2\n",
        "    y1 = j // 2\n",
        "    y2 = j % 2\n",
        "    p_XY[i,j] = p_XY_1[x1,y1] * p_XY_2[x2,y2]\n",
        "\n",
        "lambdas_true = np.zeros((4,))\n",
        "X_true = np.zeros([4,4])\n",
        "Y_true = np.zeros([4,4])\n",
        "\n",
        "for k in range(4):\n",
        "  w1 = k // 2\n",
        "  w2 = k % 2\n",
        "  lambdas_true[k] = lambdas_1[w1] * lambdas_2[w2]\n",
        "  for i in range(4):\n",
        "    x1 = i // 2\n",
        "    x2 = i % 2\n",
        "    X_true[k,i] = X_1[w1,x1] * X_2[w2,x2]\n",
        "  for j in range(4):\n",
        "    y1 = j // 2\n",
        "    y2 = j % 2\n",
        "    Y_true[k,j] = Y_1[w1,y1] * Y_2[w2,y2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ehUhcw1kwT",
        "outputId": "5e914270-a63b-49e3-ed9a-b3a249c4365d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12, 0.08, 0.03, 0.02],\n",
              "       [0.08, 0.12, 0.02, 0.03],\n",
              "       [0.03, 0.02, 0.12, 0.08],\n",
              "       [0.02, 0.03, 0.08, 0.12]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X_true.T @ np.diag(lambdas_true) @ Y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkI-EFaO10gT",
        "outputId": "28b876f9-2fed-4e8c-f1df-7b2d8b0f9c91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12, 0.08, 0.03, 0.02],\n",
              "       [0.08, 0.12, 0.02, 0.03],\n",
              "       [0.03, 0.02, 0.12, 0.08],\n",
              "       [0.02, 0.03, 0.08, 0.12]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "p_XY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UtCvZcc2TuN",
        "outputId": "4ee484d4-7b15-43e9-e3c9-de6a7b0b47e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C_1 =  0.70590, C_2 =  0.26997\n"
          ]
        }
      ],
      "source": [
        "def binary_entropy(a):\n",
        "  return - (a * math.log2(a) + (1-a) * math.log2(1-a))\n",
        "\n",
        "def eval_Wyners_commom_info_DSBS(alpha):\n",
        "  a = 0.5 * (1 - math.sqrt(1-2*alpha))\n",
        "  return 1 + binary_entropy(alpha) - 2 * binary_entropy(a)\n",
        "\n",
        "C_1 = eval_Wyners_commom_info_DSBS(alpha_1)\n",
        "C_2 = eval_Wyners_commom_info_DSBS(alpha_2)\n",
        "print(f'C_1 = {C_1: .5f}, C_2 = {C_2: .5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJkLgbyK3MjK",
        "outputId": "58a80ea7-6bfa-4e32-9d1d-23e324b91d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C_1_est =  0.70590, C_2_est =  0.26997\n"
          ]
        }
      ],
      "source": [
        "def eval_entropy(dist):\n",
        "  return - np.sum(np.where(dist != 0, dist * np.log2(dist), 0))\n",
        "\n",
        "def evaluate_cond_entropy(lambdas, X, Y):\n",
        "    q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    res = (-1) * np.sum(lambdas * (np.sum(np.where(X != 0, X * np.log2(X), 0), axis=1) + np.sum(np.where(Y != 0, Y * np.log2(Y), 0), axis=1)))\n",
        "    return res\n",
        "\n",
        "def estimate_Wyners_commom_info(lambdas, X, Y):\n",
        "  q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "  return eval_entropy(q_joint) - evaluate_cond_entropy(lambdas, X, Y)\n",
        "\n",
        "def evaluate_D_KL(lambdas, X, Y, p_joint):\n",
        "    q_joint = X.T @ np.diag(lambdas) @ Y\n",
        "    p_joiny_entropy = - np.sum(np.where(p_joint != 0,p_joint * np.log(p_joint), 0))\n",
        "    res = (-1) * np.sum(np.where(p_joint != 0, p_joint * np.log(q_joint), 0)) - p_joiny_entropy\n",
        "    return res\n",
        "\n",
        "C_1_est = estimate_Wyners_commom_info(lambdas_1, X_1, Y_1)\n",
        "C_2_est = estimate_Wyners_commom_info(lambdas_2, X_2, Y_2)\n",
        "\n",
        "print(f'C_1_est = {C_1_est: .5f}, C_2_est = {C_2_est: .5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8bXOeb353Xd",
        "outputId": "60056eca-9e8f-428d-f6ef-eec942a9a26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C_XY_true =  0.975876\n",
            "C_XY_est =  0.975876\n"
          ]
        }
      ],
      "source": [
        "C_XY_true = C_1 + C_2\n",
        "print(f'C_XY_true = {C_XY_true: .6f}')\n",
        "C_XY_est = estimate_Wyners_commom_info(lambdas_true, X_true, Y_true)\n",
        "print(f'C_XY_est = {C_XY_est: .6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSBzjEEXONeR",
        "outputId": "7931d38b-614e-42fe-f37b-3766df2d6f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    : distr proximity =  0.02922143, objective loss = -2.383060, consrt_aff_grad = 0.069163, obj_aff_grad = 1.482762, beta =  0.10000000, lr =  0.010000\n",
            "100  : distr proximity =  0.01295000, objective loss = -2.772589, consrt_aff_grad = 0.000617, obj_aff_grad = 0.171488, beta =  0.10000000, lr =  5.120000, step size = 0.00019340\n",
            "200  : distr proximity =  0.01295000, objective loss = -2.772589, consrt_aff_grad = 0.000617, obj_aff_grad = 0.171489, beta =  0.05000000, lr =  10.240000, step size = 0.00015924\n",
            "300  : distr proximity =  0.01295000, objective loss = -2.772589, consrt_aff_grad = 0.000618, obj_aff_grad = 0.171489, beta =  0.02500000, lr =  10.240000, step size = 0.00023121\n",
            "400  : distr proximity =  0.01178978, objective loss = -2.739218, consrt_aff_grad = 0.010417, obj_aff_grad = 0.345476, beta =  0.01250000, lr =  20.480000, step size = 0.04801394\n",
            "500  : distr proximity =  0.00217250, objective loss = -2.247703, consrt_aff_grad = 0.011268, obj_aff_grad = 1.746409, beta =  0.00625000, lr =  20.480000, step size = 0.00029148\n",
            "600  : distr proximity =  0.00182700, objective loss = -2.173590, consrt_aff_grad = 0.006233, obj_aff_grad = 1.927461, beta =  0.00312500, lr =  20.480000, step size = 0.00026505\n",
            "700  : distr proximity =  0.00173309, objective loss = -2.133391, consrt_aff_grad = 0.003282, obj_aff_grad = 2.028086, beta =  0.00156250, lr =  20.480000, step size = 0.00014369\n",
            "800  : distr proximity =  0.00170846, objective loss = -2.112334, consrt_aff_grad = 0.001686, obj_aff_grad = 2.081665, beta =  0.00078125, lr =  10.240000, step size = 0.00017948\n",
            "900  : distr proximity =  0.00170214, objective loss = -2.101537, consrt_aff_grad = 0.000857, obj_aff_grad = 2.109397, beta =  0.00039063, lr =  10.240000, step size = 0.00017046\n",
            "1000 : distr proximity =  0.00133907, objective loss = -1.946941, consrt_aff_grad = 0.001591, obj_aff_grad = 6.624135, beta =  0.00019531, lr =  40.960000, step size = 0.02486289\n",
            "1100 : distr proximity =  0.00125056, objective loss = -1.877607, consrt_aff_grad = 0.000734, obj_aff_grad = 9.050148, beta =  0.00009766, lr =  1.280000, step size = 0.00017057\n",
            "1200 : distr proximity =  0.00075874, objective loss = -1.807077, consrt_aff_grad = 0.001834, obj_aff_grad = 12.124622, beta =  0.00004883, lr =  10.240000, step size = 0.00765227\n",
            "1300 : distr proximity =  0.00047351, objective loss = -1.758666, consrt_aff_grad = 0.001340, obj_aff_grad = 13.166582, beta =  0.00002441, lr =  10.240000, step size = 0.00071318\n",
            "1400 : distr proximity =  0.00047002, objective loss = -1.759539, consrt_aff_grad = 0.001480, obj_aff_grad = 13.163103, beta =  0.00001221, lr =  10.240000, step size = 0.00032022\n",
            "1500 : distr proximity =  0.00046940, objective loss = -1.759643, consrt_aff_grad = 0.001506, obj_aff_grad = 13.162327, beta =  0.00000610, lr =  10.240000, step size = 0.00030729\n",
            "1600 : distr proximity =  0.00046927, objective loss = -1.759636, consrt_aff_grad = 0.001513, obj_aff_grad = 13.162109, beta =  0.00000305, lr =  40.960000, step size = 0.00035550\n",
            "1700 : distr proximity =  0.00046923, objective loss = -1.759622, consrt_aff_grad = 0.001513, obj_aff_grad = 13.162001, beta =  0.00000153, lr =  20.480000, step size = 0.00011678\n",
            "1800 : distr proximity =  0.00046922, objective loss = -1.759611, consrt_aff_grad = 0.001513, obj_aff_grad = 13.161964, beta =  0.00000076, lr =  20.480000, step size = 0.00017246\n",
            "1900 : distr proximity =  0.00046922, objective loss = -1.759605, consrt_aff_grad = 0.001513, obj_aff_grad = 13.161947, beta =  0.00000038, lr =  20.480000, step size = 0.00011894\n",
            "2000 : distr proximity =  0.00046922, objective loss = -1.759602, consrt_aff_grad = 0.001513, obj_aff_grad = 13.161938, beta =  0.00000019, lr =  10.240000, step size = 0.00024738\n",
            "2100 : distr proximity =  0.00046923, objective loss = -1.759600, consrt_aff_grad = 0.001514, obj_aff_grad = 13.161936, beta =  0.00000010, lr =  20.480000, step size = 0.00034256\n",
            "2200 : distr proximity =  0.00046922, objective loss = -1.759601, consrt_aff_grad = 0.001514, obj_aff_grad = 13.161935, beta =  0.00000010, lr =  20.480000, step size = 0.00023718\n",
            "2300 : distr proximity =  0.00046923, objective loss = -1.759600, consrt_aff_grad = 0.001514, obj_aff_grad = 13.161935, beta =  0.00000010, lr =  40.960000, step size = 0.00027818\n",
            "2400 : distr proximity =  0.00046922, objective loss = -1.759601, consrt_aff_grad = 0.001513, obj_aff_grad = 13.161935, beta =  0.00000010, lr =  20.480000, step size = 0.00009630\n",
            "2500 : distr proximity =  0.00046922, objective loss = -1.759601, consrt_aff_grad = 0.001513, obj_aff_grad = 13.161935, beta =  0.00000010, lr =  10.240000, step size = 0.00020105\n",
            "C_XY_estim =  1.153993, C_XY_true =  0.975876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-6a1c7111638d>:6: RuntimeWarning: divide by zero encountered in log2\n",
            "  res = (-1) * np.sum(lambdas * (np.sum(np.where(X != 0, X * np.log2(X), 0), axis=1) + np.sum(np.where(Y != 0, Y * np.log2(Y), 0), axis=1)))\n",
            "<ipython-input-9-6a1c7111638d>:6: RuntimeWarning: invalid value encountered in multiply\n",
            "  res = (-1) * np.sum(lambdas * (np.sum(np.where(X != 0, X * np.log2(X), 0), axis=1) + np.sum(np.where(Y != 0, Y * np.log2(Y), 0), axis=1)))\n"
          ]
        }
      ],
      "source": [
        "p_joint=p_XY\n",
        "s=4\n",
        "n_iters=2500\n",
        "params = {}\n",
        "params['lr_init']=0.01\n",
        "params['beta_init']=1e-1\n",
        "params['beta_decay_factor']=2\n",
        "params['beta_decay_every_iters']=100\n",
        "params['beta_min'] = 1e-7\n",
        "params['tol']=1e-8\n",
        "params['eps']=1e-8\n",
        "params['print_every'] = 100\n",
        "params['alpha'] = 1\n",
        "params['loss_type'] = 'l2'\n",
        "params['optim'] = 'GD'\n",
        "params['tau1'] = 1\n",
        "params['tau2'] = 10\n",
        "\n",
        "solver_GD = WynerProblemSolver(p_joint, s, n_iters, **params)\n",
        "lambdas, X, Y = solver_GD.train()\n",
        "\n",
        "cond_entropy = evaluate_cond_entropy(lambdas, X, Y)\n",
        "C_XY_estim = estimate_Wyners_commom_info(lambdas, X, Y)\n",
        "print(f'C_XY_estim = {C_XY_estim: .6f}, C_XY_true = {C_XY_true: .6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKEiIT3USi8r",
        "outputId": "d749d036-1f6e-4d58-d890-590712572852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.280997\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.258773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-6a1c7111638d>:6: RuntimeWarning: divide by zero encountered in log2\n",
            "  res = (-1) * np.sum(lambdas * (np.sum(np.where(X != 0, X * np.log2(X), 0), axis=1) + np.sum(np.where(Y != 0, Y * np.log2(Y), 0), axis=1)))\n",
            "<ipython-input-9-6a1c7111638d>:6: RuntimeWarning: invalid value encountered in multiply\n",
            "  res = (-1) * np.sum(lambdas * (np.sum(np.where(X != 0, X * np.log2(X), 0), axis=1) + np.sum(np.where(Y != 0, Y * np.log2(Y), 0), axis=1)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182463\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.332012\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269962\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182427\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00169999, C_XY_estim - C_XY_true = -0.269078\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269972\n",
            "proximity =  0.00046951, C_XY_estim - C_XY_true =  0.178170\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269971\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182464\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.056013\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.266563\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182426\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269974\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.275247\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.339938\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.227344\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.184872\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182431\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269971\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.387096\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269974\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269974\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269974\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269974\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.387743\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.021887\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182431\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182433\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269978\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269971\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.007279\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182427\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269983\n",
            "proximity =  0.00000000, C_XY_estim - C_XY_true =  0.371432\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269965\n",
            "proximity =  0.00050464, C_XY_estim - C_XY_true =  0.182463\n",
            "proximity =  0.00046922, C_XY_estim - C_XY_true =  0.178118\n",
            "proximity =  0.00170000, C_XY_estim - C_XY_true = -0.269971\n"
          ]
        }
      ],
      "source": [
        "p_joint=p_XY\n",
        "s=4\n",
        "n_iters=3000\n",
        "params = {}\n",
        "params['lr_init']=0.01\n",
        "params['beta_init']=3e-2\n",
        "params['beta_decay_factor']=2\n",
        "params['beta_decay_every_iters']=100\n",
        "params['beta_min'] = 1e-9\n",
        "params['tol']=1e-8\n",
        "params['eps']=1e-8\n",
        "params['print_every'] = None\n",
        "params['alpha'] = 1\n",
        "params['loss_type'] = 'l2'\n",
        "params['optim'] = 'GD'\n",
        "params['tau1'] = 1\n",
        "params['tau2'] = 10\n",
        "\n",
        "C_XY_estim = []\n",
        "proximity_final = []\n",
        "\n",
        "for _ in range(50):\n",
        "  solver_GD = WynerProblemSolver(p_joint, s, n_iters, **params)\n",
        "  lambdas, X, Y = solver_GD.train()\n",
        "  C_XY_estim.append(estimate_Wyners_commom_info(lambdas, X, Y))\n",
        "  proximity_final.append(solver_GD.eval_L2_loss(lambdas, X, Y))\n",
        "  print(f'proximity = {proximity_final[-1]: .8f}, C_XY_estim - C_XY_true = {C_XY_estim[-1] - C_XY_true: .6f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "obI516N1iXw-"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}